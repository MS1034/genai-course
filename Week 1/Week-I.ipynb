{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 News Groups\n",
    "\n",
    "The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, organized into 20 different categories. This dataset is widely used for experimenting with text classification and clustering algorithms. It was originally collected for a text classification project and has since become a standard benchmark in the field of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Download the 20 Newsgroups dataset\n",
    "newsgroups_dataset = fetch_20newsgroups(subset='all')\n",
    "categories = newsgroups_dataset.target_names\n",
    "\n",
    "print(categories)\n",
    "\n",
    "# Access the data\n",
    "print(newsgroups_dataset.data[0])  # Print the first news article\n",
    "print(newsgroups_dataset.target[0])  # Print the target of the first news article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review Dataset\n",
    "The Amazon Reviews dataset, specifically the `amazon_polarity` version, is a collection of product reviews from Amazon. This dataset is designed for binary sentiment classification, where the goal is to classify reviews as either positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download the Amazon reviews dataset\n",
    "amazon_dataset = load_dataset('amazon_polarity')\n",
    "print(type(dataset))\n",
    "\n",
    "# Access the data\n",
    "print(dataset['train'][0])  # Print the first review in the training set\n",
    "print(dataset['test'][0])  # Print the first review in the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "The `get_wordnet_pos` function is designed to convert the part-of-speech (POS) tags generated by NLTK's pos_tag function into a format that is compatible with the `WordNetLemmatizer`. `WordNetLemmatizer` requires specific POS tags to perform accurate lemmatization, and this function facilitates that conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Convert POS tag to format that WordNetLemmatizer can use.\n",
    "    \n",
    "    Parameters:\n",
    "    word (str): The word for which to determine the POS tag.\n",
    "    \n",
    "    Returns:\n",
    "    str: The WordNet POS tag corresponding to the first letter of the NLTK POS tag.\n",
    "         Defaults to 'n' for noun if no corresponding tag is found.\n",
    "    \n",
    "    Example:\n",
    "    >>> get_wordnet_pos('running')\n",
    "    'v'\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "The `preprocess_text` function is designed to clean and prepare text data for natural language processing tasks. It combines several preprocessing steps to transform the input text into a more uniform and analyzable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by performing the following steps:\n",
    "    1. Convert to lowercase.\n",
    "    2. Remove punctuation.\n",
    "    3. Tokenize the text.\n",
    "    4. Remove stopwords.\n",
    "    5. Lemmatize the tokens with POS tagging.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    str: The preprocessed text as a single string of tokens.\n",
    "    \n",
    "    Example:\n",
    "    >>> preprocess_text(\"Running and jumping are fun activities!\")\n",
    "    'run jump fun activity'\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the text with POS tagging\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for article in newsgroups_dataset.data:\n",
    "    preprocessed_article=preprocess_text(article)\n",
    "    print(preprocessed_article)\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in amazon_dataset[\"train\"]:\n",
    "    print(preprocess_text(review))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# def  extract_feature(processed_documents):\n",
    "    # vectorizer = CountVectorizer()\n",
    "\n",
    "    # # Fit the model and transform the documents into a BoW representation\n",
    "    # X = vectorizer.fit_transform(processed_documents)\n",
    "\n",
    "    # # Convert the BoW matrix to an array for easier understanding\n",
    "    # bow_array = X.toarray()\n",
    "\n",
    "    # # Get the feature names (i.e., words)\n",
    "    # feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # # Print the BoW representation\n",
    "    # print(\"Feature Names (Vocabulary):\")\n",
    "    # print(feature_names)\n",
    "    # print(\"\\nBag-of-Words Array:\")\n",
    "    # print(bow_array)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
