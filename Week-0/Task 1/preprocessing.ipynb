{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "69016190-5841-4d7c-95cf-83c91ccb07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c54c3597-1c44-412b-9d81-73253afe1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "f732e804-c905-4246-8261-1f9343fb8274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f51852-679c-4e27-ac25-d5783f5b53db",
   "metadata": {},
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "e3e6245c-9400-4d4b-a4ac-abc0a0b44d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo  '"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][5].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "d04bd68d-008d-4557-be20-180428edfb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "91ca5ceb-526a-4f07-be0c-bf1d4ec5381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "4337fcd1-5ef3-49d1-b998-e5790a3dc7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.√∞¬ü¬ò...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use...\n",
       "7   8      0  the next school year is the year for exams.√∞¬ü¬ò...\n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr..."
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747730c-d766-479b-9f22-11bdbc2afa52",
   "metadata": {},
   "source": [
    "# Remove tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "e8dd4e1b-cdf2-48bc-b14f-7598504de721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "8f994ad6-9ffc-4780-a56c-156e8cf89b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "505dd797-1eca-4e14-a976-3e01dc226891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen\n"
     ]
    }
   ],
   "source": [
    "print(remove_tags(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b2088b26-3a19-4c86-8f42-2d4b8d438b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "0a5abc82-79b5-4700-9ea3-09f6e7612751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#model   i love u take with u all the time in ur√∞\\x9f\\x93¬±!!! √∞\\x9f\\x98\\x99√∞\\x9f\\x98\\x8e√∞\\x9f\\x91\\x84√∞\\x9f\\x91\\x85√∞\\x9f\\x92¬¶√∞\\x9f\\x92¬¶√∞\\x9f\\x92¬¶  '"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada70b32-acde-4f86-a03f-62d399ee0695",
   "metadata": {},
   "source": [
    "# Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "460be3d7-36d1-4c23-82a3-a31da3e8ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(url):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S')\n",
    "    return pattern.sub(r'', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "76bfbdfa-a646-4f06-9e1e-31683937fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'this is my scholar profile, check it out: https://scholar.google.com/citations?user=WM9A5m4AAAAJ&hl=en&oi=sra'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "62b0ab32-a535-4312-8783-4038116d4126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my scholar profile, check it out: \n"
     ]
    }
   ],
   "source": [
    "print(remove_url(url1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc36b0-351e-490a-8c64-2ead908004a8",
   "metadata": {},
   "source": [
    "# Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d5404cad-0771-4222-839b-1e7fedb67ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "58900afe-8c48-4125-ada6-4c512305caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "8b7f93ed-7f05-4b3b-85a8-d6179bea0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "aca07c16-d9f4-46f1-ba5c-42811719c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'is. this/ the? string/ with} any punctuation?!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "09e9194f-ee22-4995-afc6-a63c01e5bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is this the string with any punctuation\n"
     ]
    }
   ],
   "source": [
    "print(remove_punctuation(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "56232570-4845-4678-bb57-c94d86eeef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation2(text):\n",
    "    return text.translate(str.maketrans('','', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "82adc910-226c-450c-82ac-ce6ab37b04c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is this the string with any punctuation'"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation2(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "220c1fbb-40b3-4b63-957c-d9294a50559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "dcfd94d9-4298-44df-b661-a4b3b4ad301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>322</td>\n",
       "      <td>1</td>\n",
       "      <td>@user \"the dying of the light\"  village green/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15810</th>\n",
       "      <td>15811</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user  @user  hope above is getting emai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>5166</td>\n",
       "      <td>0</td>\n",
       "      <td>good morning bordeaux #miroirdeau #bordeauxmav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12926</th>\n",
       "      <td>12927</td>\n",
       "      <td>0</td>\n",
       "      <td>zelda breath of the wild! #e32016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>826</td>\n",
       "      <td>0</td>\n",
       "      <td>heading to #graduation @user   #mechtechplumbi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "321      322      1  @user \"the dying of the light\"  village green/...\n",
       "15810  15811      0  @user @user  @user  hope above is getting emai...\n",
       "5165    5166      0  good morning bordeaux #miroirdeau #bordeauxmav...\n",
       "12926  12927      0                zelda breath of the wild! #e32016  \n",
       "825      826      0  heading to #graduation @user   #mechtechplumbi..."
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "1ad4c8c0-1101-4151-95bf-f8c3c910616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['tweet'] = df2['tweet'].apply(remove_punctuation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "c7b384c6-b26d-4bb1-9f71-6c76c3e3a9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31679</th>\n",
       "      <td>31680</td>\n",
       "      <td>0</td>\n",
       "      <td>businessgoals √∞¬ü¬ë¬ë√∞¬ü¬è¬Ü√∞¬ü¬í¬∞√∞¬ü¬í¬≥ √¢¬ö¬†√Ø¬∏¬èwebsite u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27667</th>\n",
       "      <td>27668</td>\n",
       "      <td>0</td>\n",
       "      <td>the cheeto thing worked out great for glennbec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14758</th>\n",
       "      <td>14759</td>\n",
       "      <td>0</td>\n",
       "      <td>when sum body come up to all hype u have a who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>5631</td>\n",
       "      <td>0</td>\n",
       "      <td>user living on a street with 10 extra trees c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16267</th>\n",
       "      <td>16268</td>\n",
       "      <td>0</td>\n",
       "      <td>user user thanks for the follow   ff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "31679  31680      0  businessgoals √∞¬ü¬ë¬ë√∞¬ü¬è¬Ü√∞¬ü¬í¬∞√∞¬ü¬í¬≥ √¢¬ö¬†√Ø¬∏¬èwebsite u...\n",
       "27667  27668      0  the cheeto thing worked out great for glennbec...\n",
       "14758  14759      0  when sum body come up to all hype u have a who...\n",
       "5630    5631      0   user living on a street with 10 extra trees c...\n",
       "16267  16268      0               user user thanks for the follow   ff"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8b404-427d-4e89-a926-90a116efab43",
   "metadata": {},
   "source": [
    "# Replace Short Hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "a892f3d7-94c2-4881-98c3-06fa3b99b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AFAIK': 'As Far As I Know', 'AFK': 'Away From Keyboard', 'ASAP': 'As Soon As Possible', 'ATK': 'At The Keyboard', 'ATM': 'At The Moment', 'A3': 'Anytime, Anywhere, Anyplace', 'BAK': 'Back At Keyboard', 'BBL': 'Be Back Later', 'BBS': 'Be Back Soon', 'BFN': 'Bye For Now', 'B4N': 'Bye For Now', 'BRB': 'Be Right Back', 'BRT': 'Be Right There', 'BTW': 'By The Way', 'B4': 'Before', 'CU': 'See You', 'CUL8R': 'See You Later', 'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FC': 'Fingers Crossed', 'FWIW': \"For What It's Worth\", 'FYI': 'For Your Information', 'GAL': 'Get A Life', 'GG': 'Good Game', 'GN': 'Good Night', 'GMTA': 'Great Minds Think Alike', 'GR8': 'Great!', 'G9': 'Genius', 'IC': 'I See', 'ICQ': 'I Seek you (also a chat program)', 'ILU': 'ILU: I Love You', 'IMHO': 'In My Honest/Humble Opinion', 'IMO': 'In My Opinion', 'IOW': 'In Other Words', 'IRL': 'In Real Life', 'KISS': 'Keep It Simple, Stupid', 'LDR': 'Long Distance Relationship', 'LMAO': 'Laugh My A.. Off', 'LOL': 'Laughing Out Loud', 'LTNS': 'Long Time No See', 'L8R': 'Later', 'MTE': 'My Thoughts Exactly', 'M8': 'Mate', 'NRN': 'No Reply Necessary', 'OIC': 'Oh I See', 'PITA': 'Pain In The A..', 'PRT': 'Party', 'PRW': 'Parents Are Watching', 'ROFL': 'Rolling On The Floor Laughing', 'ROFLOL': 'Rolling On The Floor Laughing Out Loud', 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off', 'SK8': 'Skate', 'STATS': 'Your sex and age', 'ASL': 'Age, Sex, Location', 'THX': 'Thank You', 'TTFN': 'Ta-Ta For Now!', 'TTYL': 'Talk To You Later', 'U': 'You', 'U2': 'You Too', 'U4E': 'Yours For Ever', 'WB': 'Welcome Back', 'WTF': 'What The F...', 'WTG': 'Way To Go!', 'WUF': 'Where Are You From?', 'W8': 'Wait...', '7K': 'Sick:-D Laugher'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('slang.txt', 'r') as file:\n",
    "    data_dict = {}\n",
    "\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line and '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            data_dict[key] = value\n",
    "\n",
    "print(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "7b4a12a4-b14f-4529-93cb-22c6aabc8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text =[]\n",
    "    for c in text.split():\n",
    "        if c.upper() in data_dict:\n",
    "            new_text.append(data_dict[c.upper()])\n",
    "        else:\n",
    "            new_text.append(c)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "05847a62-5b03-4046-a2fb-2c7a25856026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In My Honest/Humble Opinion he is the best'"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('IMHO he is the best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ece92-6202-4412-9ee6-3c12c38d6ac8",
   "metadata": {},
   "source": [
    "# Correct Incorrect words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "dfbffce3-2a29-47d7-8d12-2849380378a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install TextBlob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "588ea888-3ab3-462e-a51f-aca9958bf3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model is a computational model notable for its ability to active general-purpose language generation and other natural language processing tasks such as classification.'"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorr_text = \"A large language model is a computational model notablee for its aability to achiv general-purpose language generation and other natural language processing tasks such as classification.\"\n",
    "textBlb = TextBlob(incorr_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282fe1f-baa9-4e81-9bfa-e49627c75270",
   "metadata": {},
   "source": [
    "# Stop words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "f0e33179-e981-4c14-ba1c-a82444426867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install nltk==3.5\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "43606994-5c30-47a8-a703-fca71ce8b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "eac892c2-bc61-4faf-ab35-a43917a7c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/subhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')     #to download the necessary stopwords data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "2c5f2282-fc68-4b59-9adb-ff3e87eb9890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "e6c59d4a-d1b3-4254-91e7-3587e36df02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "1238546f-b998-4032-b6c6-fb3a7cbbf4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bihday  majesty'"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(df['tweet'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc89a64-77f7-4a9d-92a5-64731564de19",
   "metadata": {},
   "source": [
    "# Emoji Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "b4c0383e-a407-4bd0-9ca0-e91c3bb35a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emotions\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols/Pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport and map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "        u\"\\u2702-\\u27B0\"          # Other symbols\n",
    "        u\"\\u24C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "b85956aa-4741-45e3-a012-132c80636f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is \n"
     ]
    }
   ],
   "source": [
    "result = remove_emoji(\"Python is üî•\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "20b74cf0-8bb0-435e-8665-95e762b5381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "6056b471-8c93-4221-9a32-450b8e507451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji \n",
    "print(emoji.demojize('python is üî•'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd5a58-d279-4cd0-bccd-5a7216ff8a46",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "625640d8-d274-4bab-a5a7-46f076bfdcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'UK']"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1= 'I am going to UK'\n",
    "sentence1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "3af4fdba-3585-4b56-9807-f8d9b9669024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to UK', ' I will stay there for 10 days', ' Good Bye!']"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2=\"I am going to UK. I will stay there for 10 days. Good Bye!\"\n",
    "sentence2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "7e5ae015-f45c-4c0b-be16-2eea282ed812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is your name? i think i have seen you somewhere']"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence3=\"what is your name? i think i have seen you somewhere\"\n",
    "sentence3.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "73023931-d923-466b-8df9-e5c4e0e13f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_65144/1343377629.py:4: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokens = re.findall(\"[\\w']+\", snt4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'UK']"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use regex\n",
    "import re\n",
    "snt4= 'I am going to UK'\n",
    "tokens = re.findall(\"[\\w']+\", snt4)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "ce129321-92c2-4943-8255-8029317e761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/subhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download the 'punkt' tokenizer model\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "8cc63574-639d-4e3f-9914-41fb7224f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "efd86f2e-20c2-4653-b39a-bea4cba46d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = \"Muad Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It is shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "c3ed86b1-d94c-4408-a217-2fa0f7b474ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Muad Dib learned rapidly because his first training was in how to learn.',\n",
       " 'And the first lesson of all was the basic trust that he could learn.',\n",
       " 'It is shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.']"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "72c8856f-f822-41c1-a105-a5b6172b547b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Muad',\n",
       " 'Dib',\n",
       " 'learned',\n",
       " 'rapidly',\n",
       " 'because',\n",
       " 'his',\n",
       " 'first',\n",
       " 'training',\n",
       " 'was',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'of',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'trust',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'learn',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'shocking',\n",
       " 'to',\n",
       " 'find',\n",
       " 'how',\n",
       " 'many',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'they',\n",
       " 'can',\n",
       " 'learn',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'many',\n",
       " 'more',\n",
       " 'believe',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'difficult',\n",
       " '.']"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "64ad7d97-f37e-4748-9bae-90ef1a7dd661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/subhan/miniconda3/lib/python3.12/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/subhan/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/subhan/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/subhan/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/subhan/miniconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4f528c4d-fadf-4d28-bf0d-e9fe0fa2510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "5538e1e6-4f78-4cf1-b092-49d719f07e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m326.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/subhan/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/subhan/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/subhan/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/subhan/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/subhan/miniconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/subhan/miniconda3/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/subhan/miniconda3/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "3d434d5e-09bf-4732-bd0d-f7f69973be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0136bb7a-3257-439f-9983-466ae093e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "953d8a61-aee4-4e11-b52f-4e38f4ac7da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muad\n",
      "Dib\n",
      "learned\n",
      "rapidly\n",
      "because\n",
      "his\n",
      "first\n",
      "training\n",
      "was\n",
      "in\n",
      "how\n",
      "to\n",
      "learn\n",
      ".\n",
      "And\n",
      "the\n",
      "first\n",
      "lesson\n",
      "of\n",
      "all\n",
      "was\n",
      "the\n",
      "basic\n",
      "trust\n",
      "that\n",
      "he\n",
      "could\n",
      "learn\n",
      ".\n",
      "It\n",
      "is\n",
      "shocking\n",
      "to\n",
      "find\n",
      "how\n",
      "many\n",
      "people\n",
      "do\n",
      "not\n",
      "believe\n",
      "they\n",
      "can\n",
      "learn\n",
      ",\n",
      "and\n",
      "how\n",
      "many\n",
      "more\n",
      "believe\n",
      "learning\n",
      "to\n",
      "be\n",
      "difficult\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "6c1d4214-452a-437c-bdc5-64133a19fb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.04\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('A 5km ride cost $10.04')\n",
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3f4eff2b-5054-488c-b88a-32f0eea67503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.04']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('A 5km ride cost $10.04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70829df-c1bb-470f-bdd2-3308b732a1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6f76af-fd1d-4a7e-a1b1-944bb37f4840",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "09c94211-cd43-453d-891f-73aaeded3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the words ‚Äúhelping‚Äù and ‚Äúhelper‚Äù share the root ‚Äúhelp.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "8f7509c0-5f6d-4c45-b43c-39af5a7df59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "572d0acd-e555-4869-91e7-2836a022a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ebe3eddc-3a02-4c87-97fb-966ec087e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_stemming = \"\"\"\n",
    "... The crew of the USS Discovery discovered many discoveries.\n",
    "... Discovering is what explorers do.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "3922d7ce-7ca7-4a43-afe9-bc40a18b7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_stemming) # 'walks, walking, walked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "498f375f-546e-48a2-af7a-80559c11c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "0ae2cf7e-176d-41fe-a934-5ace68024def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "26007e02-f1c5-4a2e-85fd-2d8930e39cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThose results look a little inconsistent. Why would 'Discovery' give you 'discoveri' when 'Discovering' gives you 'discov'?\\n\\nUnderstemming and overstemming are two ways stemming can go wrong:\\n\\nUnderstemming happens when two related words should be reduced to the same stem but aren‚Äôt. This is a false negative.\\nOverstemming happens when two unrelated words are reduced to the same stem even though they shouldn‚Äôt be. This is a false positive.\\n\""
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Those results look a little inconsistent. Why would 'Discovery' give you 'discoveri' when 'Discovering' gives you 'discov'?\n",
    "\n",
    "Understemming and overstemming are two ways stemming can go wrong:\n",
    "\n",
    "Understemming happens when two related words should be reduced to the same stem but aren‚Äôt. This is a false negative.\n",
    "Overstemming happens when two unrelated words are reduced to the same stem even though they shouldn‚Äôt be. This is a false positive.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bea5c9-7e3d-4b13-bdf5-85a4179efe97",
   "metadata": {},
   "source": [
    "# lemmatizing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "1ff2e8ff-a81e-459c-9182-a25758bba38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer     #lexical dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "14617bf2-e1f6-4526-afde-855c700e5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/subhan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "b246b15b-85d2-4965-abd2-21c473020f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "c38a9e69-ce77-414b-ac84-7db89b9a27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scarf'"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"scarves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "a876d00d-f9b5-4723-9246-13d9060ed5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "1607eb43-a22d-41d0-8ebe-c74880b8f37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "7aa27d42-587a-4a4e-a61d-e570df8d9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "5488618b-8c45-40f9-9746-f15ac1423044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "7564ad09-430e-4058-9fb0-f12e146498f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worst'"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\")                  #lemmatized a word that looked very different from its lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "09bbf33e-37d3-4307-aedb-df731797a73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7831c-1b49-4649-841d-c12c0e97f2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0dca0-09ce-4a28-85d1-0dde7829c9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf175f-3980-4d17-8730-f91470a12a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697ce7d-5b27-415a-8d3b-034419fe76d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb9265-cfe6-453d-9b71-c5747c91a988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079c81b-4e0c-4582-864a-c5da96ad2f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
